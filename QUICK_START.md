# Quick Start - NYC Taxi Data Pipeline

## Configura√ß√£o Atual da DAG

‚úÖ **Per√≠odo de Dados**: Maio 2025 at√© Outubro 2025
‚úÖ **Total de Meses**: 6 meses
‚úÖ **Tamanho Estimado**: ~400 MB de dados brutos
‚úÖ **Execu√ß√µes Paralelas**: M√°ximo 2

## Dados Dispon√≠veis (Verificado)

```
2025: Mai, Jun, Jul, Ago, Set, Out (6 meses)
```

‚úÖ Todos os 6 meses est√£o dispon√≠veis e verificados.

üí° **Nota**: Se precisar de mais dados hist√≥ricos, todos os meses de 2024 e in√≠cio de 2025 tamb√©m est√£o dispon√≠veis. Basta ajustar a `start_date` na DAG.

## Passo a Passo para Executar

### 0. **IMPORTANTE**: Baixar os JARs do Spark

‚ö†Ô∏è **ANTES de iniciar os containers**, voc√™ precisa baixar os JARs do Spark:

```bash
# Navegar para o diret√≥rio spark-jars
cd spark-jars

# Op√ß√£o 1: Script autom√°tico (Linux/Mac)
chmod +x download_jars.sh
./download_jars.sh

# Op√ß√£o 2: Download manual (Windows ou se o script falhar)
# PostgreSQL JDBC Driver
wget https://jdbc.postgresql.org/download/postgresql-42.6.0.jar

# Hadoop AWS
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar

# AWS SDK Bundle
wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.772/aws-java-sdk-bundle-1.12.772.jar

# Voltar para a raiz do projeto
cd ..
```

üìã Veja mais detalhes em [spark-jars/README.md](spark-jars/README.md)

### 1. Iniciar os Containers

```bash
# Navegar para o diret√≥rio do projeto
cd "Data Pipeline"

# Iniciar todos os servi√ßos (incluindo Streamlit!)
docker compose up -d --build

# Verificar se todos est√£o rodando
docker compose ps
```

**Aguarde ~2-3 minutos** para todos os servi√ßos iniciarem.

Voc√™ ter√° 5 servi√ßos rodando:
- ‚úÖ **Airflow** (orquestrador)
- ‚úÖ **PostgreSQL** (Data Warehouse)
- ‚úÖ **MinIO** (Data Lake)
- ‚úÖ **Streamlit** (Dashboard)
- ‚úÖ **MinIO MC** (setup do MinIO)

### 2. Verificar Logs do Airflow

```bash
docker compose logs -f airflow
```

Aguarde at√© ver a mensagem "Airflow is ready" ou similar, depois pressione `Ctrl+C`.

### 3. Obter Senha do Airflow

```bash
docker exec airflow cat /opt/airflow/standalone_admin_password.txt
```

Copie a senha exibida.

### 4. Acessar as Interfaces

#### üéØ Airflow UI
1. Abra: http://localhost:8080
2. Login:
   - **Username**: `admin`
   - **Password**: (senha obtida no passo 3)

#### üìä Streamlit Dashboard
- Acesse: http://localhost:8501
- Dashboard interativo com m√©tricas e visualiza√ß√µes
- Atualiza automaticamente conforme os dados s√£o processados

#### üóÑÔ∏è MinIO Console
- Acesse: http://localhost:9001
- Login: `minioadmin` / `minio@1234!`
- Visualize os dados na camada Bronze

### 5. Ativar e Executar a DAG

1. Na lista de DAGs do Airflow, localize **`nyc_taxi_data_pipeline`**
2. Clique no **toggle (switch)** √† esquerda para ativar a DAG
3. O Airflow automaticamente iniciar√° o processamento dos 6 meses
4. Voc√™ ver√° 2 execu√ß√µes rodando em paralelo

### 6. Monitorar a Execu√ß√£o

#### No Airflow
- **Grid View**: Veja todas as 6 runs mensais
- **Graph View**: Veja o progresso de cada task individual
- **Logs**: Clique em cada task para ver logs detalhados

#### No Terminal
```bash
# Ver status geral
docker exec airflow airflow dags list-runs -d nyc_taxi_data_pipeline

# Ver logs em tempo real
docker compose logs -f airflow | grep -E "ingest|transform|dbt"

# Ver logs do Streamlit
docker compose logs -f streamlit
```

### 7. Acompanhar no Dashboard Streamlit

Enquanto a DAG processa os dados:
1. Acesse http://localhost:8501
2. Use os filtros na sidebar para explorar os dados
3. O dashboard mostra:
   - üìä M√©tricas principais (viagens, receita, dist√¢ncia)
   - üìà Viagens por dia
   - üèôÔ∏è Top bairros
   - ‚è∞ Padr√µes de demanda por hora
   - üí≥ M√©todos de pagamento
   - üó∫Ô∏è Rotas mais populares
   - üí∞ An√°lise de receita e gorjetas

### 8. Consultar Dados Processados

```bash
# Conectar ao PostgreSQL
docker exec -it postgres-nyc-taxi psql -U nyc_user -d nyc_taxi_db

# Ver tabelas criadas
\dt

# Consultar dados Silver
SELECT
    partition_year,
    partition_month,
    COUNT(*) as total_trips,
    ROUND(AVG(trip_distance)::numeric, 2) as avg_distance,
    ROUND(AVG(total_amount)::numeric, 2) as avg_fare
FROM nyc_trips_silver
GROUP BY partition_year, partition_month
ORDER BY partition_year, partition_month;

# Consultar agrega√ß√µes Gold
SELECT * FROM daily_trip_stats ORDER BY trip_date DESC LIMIT 20;

# Sair
\q
```

## Tempo Estimado de Execu√ß√£o

Com 2 runs paralelas:

- **Ingest√£o**: ~5 min/m√™s ‚Üí ~15 min total
- **Transforma√ß√£o Spark**: ~10 min/m√™s ‚Üí ~30 min total
- **dbt Silver**: ~1 min/m√™s ‚Üí ~3 min total
- **dbt Gold**: ~2 min/m√™s ‚Üí ~6 min total
- **dbt Tests**: ~1 min/m√™s ‚Üí ~3 min total

**Total Estimado**: ~1 hora para processar todos os 6 meses

## Comandos √öteis Durante a Execu√ß√£o

### Pausar Temporariamente

```bash
# Pausar a DAG (para todas as novas execu√ß√µes)
docker exec airflow airflow dags pause nyc_taxi_data_pipeline

# Despausar
docker exec airflow airflow dags unpause nyc_taxi_data_pipeline
```

### Verificar Espa√ßo em Disco

```bash
# Ver tamanho dos dados no PostgreSQL
docker exec -it postgres-nyc-taxi psql -U nyc_user -d nyc_taxi_db -c "
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname IN ('public')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;"

# Ver uso de disco dos containers
docker system df -v
```

### Reiniciar Servi√ßos

```bash
# Reiniciar um servi√ßo espec√≠fico
docker compose restart airflow
docker compose restart streamlit
docker compose restart postgres-nyc-taxi

# Reiniciar todos
docker compose restart
```

### Em Caso de Erro

```bash
# Ver logs de erro do Airflow
docker compose logs airflow | grep -i error

# Ver logs de erro do Streamlit
docker compose logs streamlit | grep -i error

# Limpar e recome√ßar (CUIDADO: apaga todos os dados!)
docker compose down -v
docker compose up -d --build
```

## Acessar Documenta√ß√£o do dbt

Ap√≥s as runs completarem:

```bash
# Entrar no container do Airflow
docker exec -it airflow bash

# Navegar para dbt
cd /opt/airflow/dbt_project

# Servir documenta√ß√£o
export DBT_PROFILES_DIR=/opt/airflow/dbt_project
dbt docs serve --port 8001

# Sair do container (Ctrl+D)
```

Acesse: http://localhost:8001

## Validar Resultados

### Verificar Quantidade de Dados

```sql
-- Conectar ao banco
docker exec -it postgres-nyc-taxi psql -U nyc_user -d nyc_taxi_db

-- Verificar total de viagens por m√™s
SELECT
    partition_year || '-' || LPAD(partition_month::text, 2, '0') as year_month,
    COUNT(*) as trips,
    pg_size_pretty(pg_total_relation_size('nyc_trips_silver')) as table_size
FROM nyc_trips_silver
GROUP BY partition_year, partition_month
ORDER BY partition_year, partition_month;

-- Verificar quantos meses foram processados
SELECT COUNT(DISTINCT (partition_year, partition_month)) as months_processed
FROM nyc_trips_silver;
-- Deve retornar: 6
```

### Verificar Qualidade dos Dados

Os testes dbt j√° validam automaticamente:
- ‚úÖ trip_id √© √∫nico e n√£o-nulo
- ‚úÖ tpep_pickup_datetime n√£o √© nulo
- ‚úÖ trip_distance n√£o √© nulo
- ‚úÖ total_amount n√£o √© nulo

Verifique os resultados na UI do Airflow na task `dbt_test_data_quality`.

## Pr√≥ximos Passos

Ap√≥s a conclus√£o bem-sucedida:

1. ‚úÖ **Dados Bronze**: Armazenados no MinIO (http://localhost:9001)
2. ‚úÖ **Dados Silver**: Tabela `nyc_trips_silver` no PostgreSQL
3. ‚úÖ **Dados Gold**: Tabelas `daily_trip_stats` e `hourly_demand_patterns`
4. ‚úÖ **Dashboard**: Acesse http://localhost:8501
5. ‚úÖ **Documenta√ß√£o**: Gerada pelo dbt (http://localhost:8001)
6. ‚úÖ **Qualidade**: Validada pelos testes dbt

Voc√™ pode:
- üìä Explorar o dashboard Streamlit com diferentes filtros
- üìà Criar novos modelos dbt em `dbt_project/models/gold/`
- üß™ Adicionar mais testes em `schema.yml`
- üé® Customizar o dashboard editando `streamlit_app/app.py`
- ‚è±Ô∏è Ajustar a frequ√™ncia da DAG (mensal, semanal, etc.)

## Troubleshooting

### "ClassNotFoundException" ou erros de Spark

‚ö†Ô∏è **Causa**: JARs do Spark n√£o foram baixados
**Solu√ß√£o**: Execute o passo 0 (Download dos JARs) e reinicie:
```bash
cd spark-jars
./download_jars.sh
cd ..
docker compose restart airflow
```

### "No space left on device"
- Libere espa√ßo em disco
- Ou reduza o per√≠odo processando menos meses

### "Connection refused" ao acessar PostgreSQL
```bash
docker compose logs postgres-nyc-taxi
docker compose restart postgres-nyc-taxi
```

### Tasks falhando com "Out of Memory"
- Aumente mem√≥ria do Docker Desktop (Settings ‚Üí Resources)
- Ou reduza `max_active_runs` para 1

### Dados n√£o aparecem no PostgreSQL
- Verifique se a task `transform_data` completou com sucesso
- Verifique logs: `docker compose logs airflow | grep transformer`

### Dashboard Streamlit n√£o carrega
```bash
# Ver logs
docker compose logs streamlit

# Reiniciar
docker compose restart streamlit

# Verificar se PostgreSQL tem dados
docker exec -it postgres-nyc-taxi psql -U nyc_user -d nyc_taxi_db -c "SELECT COUNT(*) FROM nyc_trips_silver;"
```

## URLs R√°pidas

- üéØ **Airflow**: http://localhost:8080
- üìä **Streamlit Dashboard**: http://localhost:8501
- üóÑÔ∏è **MinIO Console**: http://localhost:9001
- üìö **dbt Docs**: http://localhost:8001 (ap√≥s executar `dbt docs serve`)

## Contatos e Suporte

- **Documenta√ß√£o dbt**: https://docs.getdbt.com
- **Documenta√ß√£o Airflow**: https://airflow.apache.org/docs
- **NYC TLC Data**: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
- **Streamlit Docs**: https://docs.streamlit.io
